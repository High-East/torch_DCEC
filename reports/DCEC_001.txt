Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/6
----------
Pretraining:	Epoch: [1][10/100]	Loss 0.9887 (1.1974)	
Pretraining:	Epoch: [1][20/100]	Loss 0.9325 (1.0736)	
Pretraining:	Epoch: [1][30/100]	Loss 0.9191 (1.0261)	
Pretraining:	Epoch: [1][40/100]	Loss 0.9320 (0.9997)	
Pretraining:	Epoch: [1][50/100]	Loss 0.8923 (0.9825)	
Pretraining:	Epoch: [1][60/100]	Loss 0.9112 (0.9718)	
Pretraining:	Epoch: [1][70/100]	Loss 0.9086 (0.9625)	
Pretraining:	Epoch: [1][80/100]	Loss 0.9099 (0.9556)	
Pretraining:	Epoch: [1][90/100]	Loss 0.8474 (0.9478)	
Pretraining:	Epoch: [1][100/100]	Loss 0.8597 (0.9407)	
Pretraining:	 Loss: 0.9407

Pretraining:	Epoch 2/6
----------
Pretraining:	Epoch: [2][10/100]	Loss 0.8143 (0.8415)	
Pretraining:	Epoch: [2][20/100]	Loss 0.7068 (0.7996)	
Pretraining:	Epoch: [2][30/100]	Loss 0.6336 (0.7559)	
Pretraining:	Epoch: [2][40/100]	Loss 0.7398 (0.7285)	
Pretraining:	Epoch: [2][50/100]	Loss 0.5772 (0.7092)	
Pretraining:	Epoch: [2][60/100]	Loss 0.5398 (0.6857)	
Pretraining:	Epoch: [2][70/100]	Loss 0.5024 (0.6622)	
Pretraining:	Epoch: [2][80/100]	Loss 0.4592 (0.6390)	
Pretraining:	Epoch: [2][90/100]	Loss 0.4426 (0.6171)	
Pretraining:	Epoch: [2][100/100]	Loss 0.4097 (0.5976)	
Pretraining:	 Loss: 0.5976

Pretraining:	Epoch 3/6
----------
Pretraining:	Epoch: [3][10/100]	Loss 0.4052 (0.4072)	
Pretraining:	Epoch: [3][20/100]	Loss 0.3844 (0.3994)	
Pretraining:	Epoch: [3][30/100]	Loss 0.3781 (0.3926)	
Pretraining:	Epoch: [3][40/100]	Loss 0.3617 (0.3859)	
Pretraining:	Epoch: [3][50/100]	Loss 0.3555 (0.3803)	
Pretraining:	Epoch: [3][60/100]	Loss 0.3400 (0.3749)	
Pretraining:	Epoch: [3][70/100]	Loss 0.3497 (0.3704)	
Pretraining:	Epoch: [3][80/100]	Loss 0.3305 (0.3664)	
Pretraining:	Epoch: [3][90/100]	Loss 0.3388 (0.3636)	
Pretraining:	Epoch: [3][100/100]	Loss 0.3322 (0.3607)	
Pretraining:	 Loss: 0.3607

Pretraining:	Epoch 4/6
----------
Pretraining:	Epoch: [4][10/100]	Loss 0.3240 (0.3252)	
Pretraining:	Epoch: [4][20/100]	Loss 0.3094 (0.3208)	
Pretraining:	Epoch: [4][30/100]	Loss 0.3091 (0.3181)	
Pretraining:	Epoch: [4][40/100]	Loss 0.3051 (0.3157)	
Pretraining:	Epoch: [4][50/100]	Loss 0.3041 (0.3137)	
Pretraining:	Epoch: [4][60/100]	Loss 0.3021 (0.3130)	
Pretraining:	Epoch: [4][70/100]	Loss 0.3162 (0.3155)	
Pretraining:	Epoch: [4][80/100]	Loss 0.3000 (0.3149)	
Pretraining:	Epoch: [4][90/100]	Loss 0.2967 (0.3131)	
Pretraining:	Epoch: [4][100/100]	Loss 0.2925 (0.3113)	
Pretraining:	 Loss: 0.3113

Pretraining:	Epoch 5/6
----------
Pretraining:	Epoch: [5][10/100]	Loss 0.2891 (0.2919)	
Pretraining:	Epoch: [5][20/100]	Loss 0.2885 (0.2902)	
Pretraining:	Epoch: [5][30/100]	Loss 0.2892 (0.2896)	
Pretraining:	Epoch: [5][40/100]	Loss 0.2946 (0.2888)	
Pretraining:	Epoch: [5][50/100]	Loss 0.2867 (0.2882)	
Pretraining:	Epoch: [5][60/100]	Loss 0.2846 (0.2877)	
Pretraining:	Epoch: [5][70/100]	Loss 0.2878 (0.2871)	
Pretraining:	Epoch: [5][80/100]	Loss 0.2816 (0.2867)	
Pretraining:	Epoch: [5][90/100]	Loss 0.2884 (0.2866)	
Pretraining:	Epoch: [5][100/100]	Loss 0.2789 (0.2863)	
Pretraining:	 Loss: 0.2863

Pretraining:	Epoch 6/6
----------
Pretraining:	Epoch: [6][10/100]	Loss 0.2843 (0.2791)	
Pretraining:	Epoch: [6][20/100]	Loss 0.2788 (0.2787)	
Pretraining:	Epoch: [6][30/100]	Loss 0.2816 (0.2785)	
Pretraining:	Epoch: [6][40/100]	Loss 0.2654 (0.2777)	
Pretraining:	Epoch: [6][50/100]	Loss 0.2745 (0.2780)	
Pretraining:	Epoch: [6][60/100]	Loss 0.2714 (0.2775)	
Pretraining:	Epoch: [6][70/100]	Loss 0.2704 (0.2768)	
Pretraining:	Epoch: [6][80/100]	Loss 0.2803 (0.2765)	
Pretraining:	Epoch: [6][90/100]	Loss 0.2733 (0.2761)	
Pretraining:	Epoch: [6][100/100]	Loss 0.2706 (0.2757)	
Pretraining:	 Loss: 0.2757

Pretraining complete in 0m 40s
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.2628 (0.2741)	
Epoch: [1][20/100]	Loss 0.2720 (0.2719)	
Epoch: [1][30/100]	Loss 0.2588 (0.2705)	
Epoch: [1][40/100]	Loss 0.2717 (0.2702)	
Epoch: [1][50/100]	Loss 0.2860 (0.2722)	
Epoch: [1][60/100]	Loss 0.2749 (0.2733)	
Epoch: [1][70/100]	Loss 0.2643 (0.2728)	
Epoch: [1][80/100]	Loss 0.2679 (0.2720)	
Epoch: [1][90/100]	Loss 0.2636 (0.2714)	
Epoch: [1][100/100]	Loss 0.2694 (0.2708)	
Loss: 0.2708

Epoch 2/200
----------
Epoch: [2][10/100]	Loss 0.2579 (0.2641)	
Epoch: [2][20/100]	Loss 0.2685 (0.2637)	
Epoch: [2][30/100]	Loss 0.2679 (0.2646)	
Epoch: [2][40/100]	Loss 0.2609 (0.2646)	
Epoch: [2][50/100]	Loss 0.2614 (0.2638)	
Epoch: [2][60/100]	Loss 0.2650 (0.2630)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/6
----------
Pretraining:	Epoch: [1][10/100]	Loss 0.9887 (1.3384)	
Pretraining:	Epoch: [1][20/100]	Loss 1.0085 (1.1707)	
Pretraining:	Epoch: [1][30/100]	Loss 1.0124 (1.1146)	
Pretraining:	Epoch: [1][40/100]	Loss 1.0069 (1.0864)	
Pretraining:	Epoch: [1][50/100]	Loss 0.9705 (1.0687)	
Pretraining:	Epoch: [1][60/100]	Loss 0.9861 (1.0576)	
Pretraining:	Epoch: [1][70/100]	Loss 0.9897 (1.0495)	
Pretraining:	Epoch: [1][80/100]	Loss 1.0046 (1.0428)	
Pretraining:	Epoch: [1][90/100]	Loss 0.9757 (1.0378)	
Pretraining:	Epoch: [1][100/100]	Loss 1.0102 (1.0340)	
Pretraining:	 Loss: 1.0340

Pretraining:	Epoch 2/6
----------
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7120 (0.7946)	
Epoch: [1][20/100]	Loss 0.5342 (0.7025)	
Epoch: [1][30/100]	Loss 0.4057 (0.6223)	
Epoch: [1][40/100]	Loss 0.3683 (0.5636)	
Epoch: [1][50/100]	Loss 0.3374 (0.5215)	
Epoch: [1][60/100]	Loss 0.3174 (0.4885)	
Epoch: [1][70/100]	Loss 0.3023 (0.4630)	
Epoch: [1][80/100]	Loss 0.2986 (0.4427)	
Epoch: [1][90/100]	Loss 0.2836 (0.4258)	
Epoch: [1][100/100]	Loss 0.2862 (0.4117)	
Loss: 0.4117

Epoch 2/200
----------
Epoch: [2][10/100]	Loss 0.2845 (0.2843)	
Epoch: [2][20/100]	Loss 0.2772 (0.2820)	
Epoch: [2][30/100]	Loss 0.2783 (0.2802)	
Epoch: [2][40/100]	Loss 0.2740 (0.2789)	
Epoch: [2][50/100]	Loss 0.2697 (0.2772)	
Epoch: [2][60/100]	Loss 0.2590 (0.2760)	
Epoch: [2][70/100]	Loss 0.2643 (0.2749)	
Epoch: [2][80/100]	Loss 0.2704 (0.2748)	
Epoch: [2][90/100]	Loss 0.2771 (0.2750)	
Epoch: [2][100/100]	Loss 0.2698 (0.2749)	
Loss: 0.2749

Epoch 3/200
----------
Epoch: [3][10/100]	Loss 0.2625 (0.2674)	
Epoch: [3][20/100]	Loss 0.2589 (0.2644)	
Epoch: [3][30/100]	Loss 0.2721 (0.2633)	
Epoch: [3][40/100]	Loss 0.2567 (0.2622)	
Epoch: [3][50/100]	Loss 0.2558 (0.2616)	
Epoch: [3][60/100]	Loss 0.2529 (0.2606)	
Epoch: [3][70/100]	Loss 0.2548 (0.2597)	
Epoch: [3][80/100]	Loss 0.2582 (0.2594)	
Epoch: [3][90/100]	Loss 0.2575 (0.2589)	
Epoch: [3][100/100]	Loss 0.2640 (0.2589)	
Loss: 0.2589

Epoch 4/200
----------
Epoch: [4][10/100]	Loss 0.2469 (0.2565)	
Epoch: [4][20/100]	Loss 0.2548 (0.2548)	
Epoch: [4][30/100]	Loss 0.2584 (0.2543)	
Epoch: [4][40/100]	Loss 0.2543 (0.2549)	
Epoch: [4][50/100]	Loss 0.2376 (0.2542)	
Epoch: [4][60/100]	Loss 0.2446 (0.2536)	
Epoch: [4][70/100]	Loss 0.2560 (0.2532)	
Epoch: [4][80/100]	Loss 0.2571 (0.2530)	
Epoch: [4][90/100]	Loss 0.2561 (0.2529)	
Epoch: [4][100/100]	Loss 0.2517 (0.2527)	
Loss: 0.2527

Epoch 5/200
----------
Epoch: [5][10/100]	Loss 0.2463 (0.2475)	
Epoch: [5][20/100]	Loss 0.2485 (0.2472)	
Epoch: [5][30/100]	Loss 0.2485 (0.2471)	
Epoch: [5][40/100]	Loss 0.2482 (0.2470)	
Epoch: [5][50/100]	Loss 0.2443 (0.2468)	
Epoch: [5][60/100]	Loss 0.2475 (0.2462)	
Epoch: [5][70/100]	Loss 0.2431 (0.2468)	
Epoch: [5][80/100]	Loss 0.2460 (0.2468)	
Epoch: [5][90/100]	Loss 0.2450 (0.2465)	
Epoch: [5][100/100]	Loss 0.2502 (0.2464)	
Loss: 0.2464

Epoch 6/200
----------
Epoch: [6][10/100]	Loss 0.2477 (0.2436)	
Epoch: [6][20/100]	Loss 0.2478 (0.2426)	
Epoch: [6][30/100]	Loss 0.2357 (0.2421)	
Epoch: [6][40/100]	Loss 0.2467 (0.2419)	
Epoch: [6][50/100]	Loss 0.2445 (0.2415)	
Epoch: [6][60/100]	Loss 0.2405 (0.2415)	
Epoch: [6][70/100]	Loss 0.2468 (0.2423)	
Epoch: [6][80/100]	Loss 0.2348 (0.2422)	
Epoch: [6][90/100]	Loss 0.2411 (0.2419)	
Epoch: [6][100/100]	Loss 0.2381 (0.2418)	
Loss: 0.2418

Epoch 7/200
----------
Epoch: [7][10/100]	Loss 0.2812 (0.2554)	
Epoch: [7][20/100]	Loss 0.2598 (0.2708)	
Epoch: [7][30/100]	Loss 0.2486 (0.2681)	
Epoch: [7][40/100]	Loss 0.2440 (0.2630)	
Epoch: [7][50/100]	Loss 0.2386 (0.2585)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7086 (0.8029)	
Epoch: [1][20/100]	Loss 0.5576 (0.7143)	
Epoch: [1][30/100]	Loss 0.4224 (0.6365)	
Epoch: [1][40/100]	Loss 0.3716 (0.5762)	
Epoch: [1][50/100]	Loss 0.3399 (0.5315)	
Epoch: [1][60/100]	Loss 0.3200 (0.4980)	
Epoch: [1][70/100]	Loss 0.3105 (0.4716)	
Epoch: [1][80/100]	Loss 0.2937 (0.4503)	
Epoch: [1][90/100]	Loss 0.2916 (0.4331)	
Epoch: [1][100/100]	Loss 0.2922 (0.4185)	
Loss: 0.4185

Epoch 2/200
----------
Epoch: [2][10/100]	Loss 0.2744 (0.2820)	
Epoch: [2][20/100]	Loss 0.2816 (0.2821)	
Epoch: [2][30/100]	Loss 0.2703 (0.2811)	
Epoch: [2][40/100]	Loss 0.2769 (0.2800)	
Epoch: [2][50/100]	Loss 0.2778 (0.2788)	
Epoch: [2][60/100]	Loss 0.2722 (0.2776)	
Epoch: [2][70/100]	Loss 0.2733 (0.2769)	
Epoch: [2][80/100]	Loss 0.2711 (0.2760)	
Epoch: [2][90/100]	Loss 0.2715 (0.2752)	
Epoch: [2][100/100]	Loss 0.2730 (0.2744)	
Loss: 0.2744

Epoch 3/200
----------
Epoch: [3][10/100]	Loss 0.2617 (0.2633)	
Epoch: [3][20/100]	Loss 0.2617 (0.2638)	
Epoch: [3][30/100]	Loss 0.2584 (0.2622)	
Epoch: [3][40/100]	Loss 0.2587 (0.2613)	
Epoch: [3][50/100]	Loss 0.2732 (0.2617)	
Epoch: [3][60/100]	Loss 0.2617 (0.2620)	
Epoch: [3][70/100]	Loss 0.2549 (0.2617)	
Epoch: [3][80/100]	Loss 0.2565 (0.2614)	
Epoch: [3][90/100]	Loss 0.2508 (0.2612)	
Epoch: [3][100/100]	Loss 0.2596 (0.2609)	
Loss: 0.2609

Epoch 4/200
----------
Epoch: [4][10/100]	Loss 0.2625 (0.2569)	
Epoch: [4][20/100]	Loss 0.2558 (0.2557)	
Epoch: [4][30/100]	Loss 0.2535 (0.2559)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7193 (0.8155)	
Epoch: [1][20/100]	Loss 0.5828 (0.7286)	
Epoch: [1][30/100]	Loss 0.4545 (0.6531)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.6897 (0.7752)	
Epoch: [1][20/100]	Loss 0.5133 (0.6815)	
Epoch: [1][30/100]	Loss 0.4117 (0.6028)	
Epoch: [1][40/100]	Loss 0.3619 (0.5479)	
Epoch: [1][50/100]	Loss 0.3436 (0.5075)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7331 (0.8091)	
Epoch: [1][20/100]	Loss 0.5770 (0.7251)	
Epoch: [1][30/100]	Loss 0.4460 (0.6524)	
Epoch: [1][40/100]	Loss 0.3890 (0.5926)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7004 (0.7849)	
Epoch: [1][20/100]	Loss 0.5462 (0.6953)	
Epoch: [1][30/100]	Loss 0.4234 (0.6179)	
Epoch: [1][40/100]	Loss 0.3668 (0.5605)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.6940 (0.7820)	
Epoch: [1][20/100]	Loss 0.5217 (0.6946)	
Epoch: [1][30/100]	Loss 0.4048 (0.6150)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7103 (0.7985)	
Epoch: [1][20/100]	Loss 0.5667 (0.7127)	
Epoch: [1][30/100]	Loss 0.4313 (0.6368)	
Epoch: [1][40/100]	Loss 0.3718 (0.5768)	
Epoch: [1][50/100]	Loss 0.3364 (0.5320)	
Epoch: [1][60/100]	Loss 0.3235 (0.4983)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initializing cluster centers based on K-means

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.6907 (0.7824)	
Epoch: [1][20/100]	Loss 0.5509 (0.6950)	
Epoch: [1][30/100]	Loss 0.4296 (0.6183)	
Epoch: [1][40/100]	Loss 0.3654 (0.5610)	
Epoch: [1][50/100]	Loss 0.3460 (0.5189)	
Epoch: [1][60/100]	Loss 0.3180 (0.4870)	
Epoch: [1][70/100]	Loss 0.3076 (0.4621)	
Epoch: [1][80/100]	Loss 0.2930 (0.4419)	
Epoch: [1][90/100]	Loss 0.2889 (0.4255)	
Epoch: [1][100/100]	Loss 0.2914 (0.4117)	
Loss: 0.4117

Epoch 2/200
----------
Epoch: [2][10/100]	Loss 0.2828 (0.2851)	
Epoch: [2][20/100]	Loss 0.2714 (0.2821)	
Epoch: [2][30/100]	Loss 0.2700 (0.2810)	
Epoch: [2][40/100]	Loss 0.2707 (0.2793)	
Epoch: [2][50/100]	Loss 0.2715 (0.2776)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7007 (0.7896)	
Epoch: [1][20/100]	Loss 0.5405 (0.7028)	
Epoch: [1][30/100]	Loss 0.4206 (0.6241)	
Epoch: [1][40/100]	Loss 0.3688 (0.5655)	
Epoch: [1][50/100]	Loss 0.3408 (0.5222)	
Epoch: [1][60/100]	Loss 0.3206 (0.4894)	
Epoch: [1][70/100]	Loss 0.3099 (0.4641)	
Epoch: [1][80/100]	Loss 0.2926 (0.4435)	
Epoch: [1][90/100]	Loss 0.2859 (0.4266)	
Epoch: [1][100/100]	Loss 0.2835 (0.4126)	
Loss: 0.4126

Epoch 2/200
----------
Epoch: [2][10/100]	Loss 0.2784 (0.2811)	
Epoch: [2][20/100]	Loss 0.2764 (0.2805)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6
Clustering loss weight:	0.1

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.6939 (0.7921)	
Epoch: [1][20/100]	Loss 0.5528 (0.7099)	
Epoch: [1][30/100]	Loss 0.4360 (0.6344)	
Epoch: [1][40/100]	Loss 0.3680 (0.5747)	
Epoch: [1][50/100]	Loss 0.3431 (0.5308)	
Epoch: [1][60/100]	Loss 0.3223 (0.4972)	
Epoch: [1][70/100]	Loss 0.3199 (0.4712)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6
Clustering loss weight:	0.1

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.6414 (0.7464)	
Epoch: [1][20/100]	Loss 0.4764 (0.6495)	
Epoch: [1][30/100]	Loss 0.3946 (0.5752)	
Epoch: [1][40/100]	Loss 0.3537 (0.5223)	
Epoch: [1][50/100]	Loss 0.3236 (0.4842)	
Epoch: [1][60/100]	Loss 0.3170 (0.4564)	
Epoch: [1][70/100]	Loss 0.2979 (0.4345)	
Epoch: [1][80/100]	Loss 0.2944 (0.4168)	
Epoch: [1][90/100]	Loss 0.2887 (0.4023)	
Epoch: [1][100/100]	Loss 0.2858 (0.3905)	
Loss: 0.3905

Epoch 2/200
----------
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6
Clustering loss weight:	0.1

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7197 (0.8040)	
Epoch: [1][20/100]	Loss 0.5763 (0.7212)	
Epoch: [1][30/100]	Loss 0.4515 (0.6481)	
Epoch: [1][40/100]	Loss 0.3830 (0.5884)	
Epoch: [1][50/100]	Loss 0.3458 (0.5434)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6
Clustering loss weight:	0.1
Update interval for target distribution:	3

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:


Updating target distribution:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6
Clustering loss weight:	0.1
Update interval for target distribution:	3

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:


Updating target distribution:

Updating target distribution:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6
Clustering loss weight:	0.1
Update interval for target distribution:	3

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:

Updating target distribution:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7122 (0.8339)	
Epoch: [1][20/100]	Loss 0.6027 (0.7450)	
Epoch: [1][30/100]	Loss 0.4848 (0.6751)	
Epoch: [1][40/100]	Loss 0.3970 (0.6129)	
Epoch: [1][50/100]	Loss 0.3608 (0.5650)	
Epoch: [1][60/100]	Loss 0.3316 (0.5281)	
Epoch: [1][70/100]	Loss 0.3246 (0.4988)	
Epoch: [1][80/100]	Loss 0.2978 (0.4749)	
Epoch: [1][90/100]	Loss 0.3050 (0.4554)	
Epoch: [1][100/100]	Loss 0.2906 (0.4392)	
Loss: 0.4392

Epoch 2/200
----------
Epoch: [2][10/100]	Loss 0.2885 (0.2874)	
Epoch: [2][20/100]	Loss 0.2802 (0.2859)	
Epoch: [2][30/100]	Loss 0.2758 (0.2849)	
Epoch: [2][40/100]	Loss 0.2778 (0.2831)	
Epoch: [2][50/100]	Loss 0.2733 (0.2819)	
Epoch: [2][60/100]	Loss 0.2735 (0.2811)	
Epoch: [2][70/100]	Loss 0.2747 (0.2799)	
Epoch: [2][80/100]	Loss 0.2736 (0.2787)	
Epoch: [2][90/100]	Loss 0.2724 (0.2777)	
Epoch: [2][100/100]	Loss 0.2700 (0.2770)	
Loss: 0.2770

Epoch 3/200
----------
Epoch: [3][10/100]	Loss 0.2658 (0.2678)	
Epoch: [3][20/100]	Loss 0.2605 (0.2668)	
Epoch: [3][30/100]	Loss 0.2706 (0.2664)	
Epoch: [3][40/100]	Loss 0.2624 (0.2659)	
Epoch: [3][50/100]	Loss 0.2683 (0.2651)	
Epoch: [3][60/100]	Loss 0.2687 (0.2650)	
Epoch: [3][70/100]	Loss 0.2584 (0.2647)	
Epoch: [3][80/100]	Loss 0.2665 (0.2643)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6
Clustering loss weight:	0.1
Update interval for target distribution:	1

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:

Updating target distribution:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7149 (0.7926)	
Epoch: [1][20/100]	Loss 0.5328 (0.7025)	
Epoch: [1][30/100]	Loss 0.4136 (0.6239)	
Epoch: [1][40/100]	Loss 0.3752 (0.5660)	
Epoch: [1][50/100]	Loss 0.3274 (0.5228)	
Epoch: [1][60/100]	Loss 0.3184 (0.4904)	
Epoch: [1][70/100]	Loss 0.3024 (0.4647)	
Epoch: [1][80/100]	Loss 0.2945 (0.4446)	
Epoch: [1][90/100]	Loss 0.2887 (0.4278)	
Epoch: [1][100/100]	Loss 0.2883 (0.4137)	
Loss: 0.4137


Updating target distribution:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6
Clustering loss weight:	0.1
Update interval for target distribution:	1

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:

Updating target distribution:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7063 (0.7862)	
Epoch: [1][20/100]	Loss 0.5287 (0.6962)	
Epoch: [1][30/100]	Loss 0.4137 (0.6166)	
Epoch: [1][40/100]	Loss 0.3677 (0.5592)	
Epoch: [1][50/100]	Loss 0.3363 (0.5173)	
Epoch: [1][60/100]	Loss 0.3184 (0.4856)	
Epoch: [1][70/100]	Loss 0.3075 (0.4608)	
Epoch: [1][80/100]	Loss 0.2996 (0.4408)	
Epoch: [1][90/100]	Loss 0.2879 (0.4245)	
Epoch: [1][100/100]	Loss 0.2908 (0.4108)	
Loss: 0.4108


Updating target distribution:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	300
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6
Clustering loss weight:	0.1
Update interval for target distribution:	1

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: nets/DCEC_001_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training:

Updating target distribution:
Epoch 1/200
----------
Epoch: [1][10/200]	Loss 0.6940 (0.7832)	
Epoch: [1][20/200]	Loss 0.5254 (0.6896)	
Epoch: [1][30/200]	Loss 0.4239 (0.6112)	
Epoch: [1][40/200]	Loss 0.3781 (0.5558)	
Epoch: [1][50/200]	Loss 0.3250 (0.5139)	
Epoch: [1][60/200]	Loss 0.3212 (0.4822)	
Epoch: [1][70/200]	Loss 0.3117 (0.4578)	
Epoch: [1][80/200]	Loss 0.2964 (0.4380)	
Epoch: [1][90/200]	Loss 0.2892 (0.4223)	
Epoch: [1][100/200]	Loss 0.2911 (0.4094)	
Epoch: [1][110/200]	Loss 0.2853 (0.3982)	
Epoch: [1][120/200]	Loss 0.2836 (0.3887)	
Epoch: [1][130/200]	Loss 0.2831 (0.3803)	
Epoch: [1][140/200]	Loss 0.2782 (0.3732)	
Epoch: [1][150/200]	Loss 0.2669 (0.3668)	
Epoch: [1][160/200]	Loss 0.2736 (0.3609)	
Epoch: [1][170/200]	Loss 0.2757 (0.3558)	
Epoch: [1][180/200]	Loss 0.2633 (0.3512)	
Epoch: [1][190/200]	Loss 0.2657 (0.3470)	
Epoch: [1][200/200]	Loss 0.2764 (0.3432)	
Loss: 0.3432


Updating target distribution:
