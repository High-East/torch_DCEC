Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/6
----------
Pretraining:	Epoch: [1][10/100]	Loss 0.9887 (1.1974)	
Pretraining:	Epoch: [1][20/100]	Loss 0.9325 (1.0736)	
Pretraining:	Epoch: [1][30/100]	Loss 0.9191 (1.0261)	
Pretraining:	Epoch: [1][40/100]	Loss 0.9320 (0.9997)	
Pretraining:	Epoch: [1][50/100]	Loss 0.8923 (0.9825)	
Pretraining:	Epoch: [1][60/100]	Loss 0.9112 (0.9718)	
Pretraining:	Epoch: [1][70/100]	Loss 0.9086 (0.9625)	
Pretraining:	Epoch: [1][80/100]	Loss 0.9099 (0.9556)	
Pretraining:	Epoch: [1][90/100]	Loss 0.8474 (0.9478)	
Pretraining:	Epoch: [1][100/100]	Loss 0.8597 (0.9407)	
Pretraining:	 Loss: 0.9407

Pretraining:	Epoch 2/6
----------
Pretraining:	Epoch: [2][10/100]	Loss 0.8143 (0.8415)	
Pretraining:	Epoch: [2][20/100]	Loss 0.7068 (0.7996)	
Pretraining:	Epoch: [2][30/100]	Loss 0.6336 (0.7559)	
Pretraining:	Epoch: [2][40/100]	Loss 0.7398 (0.7285)	
Pretraining:	Epoch: [2][50/100]	Loss 0.5772 (0.7092)	
Pretraining:	Epoch: [2][60/100]	Loss 0.5398 (0.6857)	
Pretraining:	Epoch: [2][70/100]	Loss 0.5024 (0.6622)	
Pretraining:	Epoch: [2][80/100]	Loss 0.4592 (0.6390)	
Pretraining:	Epoch: [2][90/100]	Loss 0.4426 (0.6171)	
Pretraining:	Epoch: [2][100/100]	Loss 0.4097 (0.5976)	
Pretraining:	 Loss: 0.5976

Pretraining:	Epoch 3/6
----------
Pretraining:	Epoch: [3][10/100]	Loss 0.4052 (0.4072)	
Pretraining:	Epoch: [3][20/100]	Loss 0.3844 (0.3994)	
Pretraining:	Epoch: [3][30/100]	Loss 0.3781 (0.3926)	
Pretraining:	Epoch: [3][40/100]	Loss 0.3617 (0.3859)	
Pretraining:	Epoch: [3][50/100]	Loss 0.3555 (0.3803)	
Pretraining:	Epoch: [3][60/100]	Loss 0.3400 (0.3749)	
Pretraining:	Epoch: [3][70/100]	Loss 0.3497 (0.3704)	
Pretraining:	Epoch: [3][80/100]	Loss 0.3305 (0.3664)	
Pretraining:	Epoch: [3][90/100]	Loss 0.3388 (0.3636)	
Pretraining:	Epoch: [3][100/100]	Loss 0.3322 (0.3607)	
Pretraining:	 Loss: 0.3607

Pretraining:	Epoch 4/6
----------
Pretraining:	Epoch: [4][10/100]	Loss 0.3240 (0.3252)	
Pretraining:	Epoch: [4][20/100]	Loss 0.3094 (0.3208)	
Pretraining:	Epoch: [4][30/100]	Loss 0.3091 (0.3181)	
Pretraining:	Epoch: [4][40/100]	Loss 0.3051 (0.3157)	
Pretraining:	Epoch: [4][50/100]	Loss 0.3041 (0.3137)	
Pretraining:	Epoch: [4][60/100]	Loss 0.3021 (0.3130)	
Pretraining:	Epoch: [4][70/100]	Loss 0.3162 (0.3155)	
Pretraining:	Epoch: [4][80/100]	Loss 0.3000 (0.3149)	
Pretraining:	Epoch: [4][90/100]	Loss 0.2967 (0.3131)	
Pretraining:	Epoch: [4][100/100]	Loss 0.2925 (0.3113)	
Pretraining:	 Loss: 0.3113

Pretraining:	Epoch 5/6
----------
Pretraining:	Epoch: [5][10/100]	Loss 0.2891 (0.2919)	
Pretraining:	Epoch: [5][20/100]	Loss 0.2885 (0.2902)	
Pretraining:	Epoch: [5][30/100]	Loss 0.2892 (0.2896)	
Pretraining:	Epoch: [5][40/100]	Loss 0.2946 (0.2888)	
Pretraining:	Epoch: [5][50/100]	Loss 0.2867 (0.2882)	
Pretraining:	Epoch: [5][60/100]	Loss 0.2846 (0.2877)	
Pretraining:	Epoch: [5][70/100]	Loss 0.2878 (0.2871)	
Pretraining:	Epoch: [5][80/100]	Loss 0.2816 (0.2867)	
Pretraining:	Epoch: [5][90/100]	Loss 0.2884 (0.2866)	
Pretraining:	Epoch: [5][100/100]	Loss 0.2789 (0.2863)	
Pretraining:	 Loss: 0.2863

Pretraining:	Epoch 6/6
----------
Pretraining:	Epoch: [6][10/100]	Loss 0.2843 (0.2791)	
Pretraining:	Epoch: [6][20/100]	Loss 0.2788 (0.2787)	
Pretraining:	Epoch: [6][30/100]	Loss 0.2816 (0.2785)	
Pretraining:	Epoch: [6][40/100]	Loss 0.2654 (0.2777)	
Pretraining:	Epoch: [6][50/100]	Loss 0.2745 (0.2780)	
Pretraining:	Epoch: [6][60/100]	Loss 0.2714 (0.2775)	
Pretraining:	Epoch: [6][70/100]	Loss 0.2704 (0.2768)	
Pretraining:	Epoch: [6][80/100]	Loss 0.2803 (0.2765)	
Pretraining:	Epoch: [6][90/100]	Loss 0.2733 (0.2761)	
Pretraining:	Epoch: [6][100/100]	Loss 0.2706 (0.2757)	
Pretraining:	 Loss: 0.2757

Pretraining complete in 0m 40s
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.2628 (0.2741)	
Epoch: [1][20/100]	Loss 0.2720 (0.2719)	
Epoch: [1][30/100]	Loss 0.2588 (0.2705)	
Epoch: [1][40/100]	Loss 0.2717 (0.2702)	
Epoch: [1][50/100]	Loss 0.2860 (0.2722)	
Epoch: [1][60/100]	Loss 0.2749 (0.2733)	
Epoch: [1][70/100]	Loss 0.2643 (0.2728)	
Epoch: [1][80/100]	Loss 0.2679 (0.2720)	
Epoch: [1][90/100]	Loss 0.2636 (0.2714)	
Epoch: [1][100/100]	Loss 0.2694 (0.2708)	
Loss: 0.2708

Epoch 2/200
----------
Epoch: [2][10/100]	Loss 0.2579 (0.2641)	
Epoch: [2][20/100]	Loss 0.2685 (0.2637)	
Epoch: [2][30/100]	Loss 0.2679 (0.2646)	
Epoch: [2][40/100]	Loss 0.2609 (0.2646)	
Epoch: [2][50/100]	Loss 0.2614 (0.2638)	
Epoch: [2][60/100]	Loss 0.2650 (0.2630)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/6
----------
Pretraining:	Epoch: [1][10/100]	Loss 0.9887 (1.3384)	
Pretraining:	Epoch: [1][20/100]	Loss 1.0085 (1.1707)	
Pretraining:	Epoch: [1][30/100]	Loss 1.0124 (1.1146)	
Pretraining:	Epoch: [1][40/100]	Loss 1.0069 (1.0864)	
Pretraining:	Epoch: [1][50/100]	Loss 0.9705 (1.0687)	
Pretraining:	Epoch: [1][60/100]	Loss 0.9861 (1.0576)	
Pretraining:	Epoch: [1][70/100]	Loss 0.9897 (1.0495)	
Pretraining:	Epoch: [1][80/100]	Loss 1.0046 (1.0428)	
Pretraining:	Epoch: [1][90/100]	Loss 0.9757 (1.0378)	
Pretraining:	Epoch: [1][100/100]	Loss 1.0102 (1.0340)	
Pretraining:	 Loss: 1.0340

Pretraining:	Epoch 2/6
----------
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7120 (0.7946)	
Epoch: [1][20/100]	Loss 0.5342 (0.7025)	
Epoch: [1][30/100]	Loss 0.4057 (0.6223)	
Epoch: [1][40/100]	Loss 0.3683 (0.5636)	
Epoch: [1][50/100]	Loss 0.3374 (0.5215)	
Epoch: [1][60/100]	Loss 0.3174 (0.4885)	
Epoch: [1][70/100]	Loss 0.3023 (0.4630)	
Epoch: [1][80/100]	Loss 0.2986 (0.4427)	
Epoch: [1][90/100]	Loss 0.2836 (0.4258)	
Epoch: [1][100/100]	Loss 0.2862 (0.4117)	
Loss: 0.4117

Epoch 2/200
----------
Epoch: [2][10/100]	Loss 0.2845 (0.2843)	
Epoch: [2][20/100]	Loss 0.2772 (0.2820)	
Epoch: [2][30/100]	Loss 0.2783 (0.2802)	
Epoch: [2][40/100]	Loss 0.2740 (0.2789)	
Epoch: [2][50/100]	Loss 0.2697 (0.2772)	
Epoch: [2][60/100]	Loss 0.2590 (0.2760)	
Epoch: [2][70/100]	Loss 0.2643 (0.2749)	
Epoch: [2][80/100]	Loss 0.2704 (0.2748)	
Epoch: [2][90/100]	Loss 0.2771 (0.2750)	
Epoch: [2][100/100]	Loss 0.2698 (0.2749)	
Loss: 0.2749

Epoch 3/200
----------
Epoch: [3][10/100]	Loss 0.2625 (0.2674)	
Epoch: [3][20/100]	Loss 0.2589 (0.2644)	
Epoch: [3][30/100]	Loss 0.2721 (0.2633)	
Epoch: [3][40/100]	Loss 0.2567 (0.2622)	
Epoch: [3][50/100]	Loss 0.2558 (0.2616)	
Epoch: [3][60/100]	Loss 0.2529 (0.2606)	
Epoch: [3][70/100]	Loss 0.2548 (0.2597)	
Epoch: [3][80/100]	Loss 0.2582 (0.2594)	
Epoch: [3][90/100]	Loss 0.2575 (0.2589)	
Epoch: [3][100/100]	Loss 0.2640 (0.2589)	
Loss: 0.2589

Epoch 4/200
----------
Epoch: [4][10/100]	Loss 0.2469 (0.2565)	
Epoch: [4][20/100]	Loss 0.2548 (0.2548)	
Epoch: [4][30/100]	Loss 0.2584 (0.2543)	
Epoch: [4][40/100]	Loss 0.2543 (0.2549)	
Epoch: [4][50/100]	Loss 0.2376 (0.2542)	
Epoch: [4][60/100]	Loss 0.2446 (0.2536)	
Epoch: [4][70/100]	Loss 0.2560 (0.2532)	
Epoch: [4][80/100]	Loss 0.2571 (0.2530)	
Epoch: [4][90/100]	Loss 0.2561 (0.2529)	
Epoch: [4][100/100]	Loss 0.2517 (0.2527)	
Loss: 0.2527

Epoch 5/200
----------
Epoch: [5][10/100]	Loss 0.2463 (0.2475)	
Epoch: [5][20/100]	Loss 0.2485 (0.2472)	
Epoch: [5][30/100]	Loss 0.2485 (0.2471)	
Epoch: [5][40/100]	Loss 0.2482 (0.2470)	
Epoch: [5][50/100]	Loss 0.2443 (0.2468)	
Epoch: [5][60/100]	Loss 0.2475 (0.2462)	
Epoch: [5][70/100]	Loss 0.2431 (0.2468)	
Epoch: [5][80/100]	Loss 0.2460 (0.2468)	
Epoch: [5][90/100]	Loss 0.2450 (0.2465)	
Epoch: [5][100/100]	Loss 0.2502 (0.2464)	
Loss: 0.2464

Epoch 6/200
----------
Epoch: [6][10/100]	Loss 0.2477 (0.2436)	
Epoch: [6][20/100]	Loss 0.2478 (0.2426)	
Epoch: [6][30/100]	Loss 0.2357 (0.2421)	
Epoch: [6][40/100]	Loss 0.2467 (0.2419)	
Epoch: [6][50/100]	Loss 0.2445 (0.2415)	
Epoch: [6][60/100]	Loss 0.2405 (0.2415)	
Epoch: [6][70/100]	Loss 0.2468 (0.2423)	
Epoch: [6][80/100]	Loss 0.2348 (0.2422)	
Epoch: [6][90/100]	Loss 0.2411 (0.2419)	
Epoch: [6][100/100]	Loss 0.2381 (0.2418)	
Loss: 0.2418

Epoch 7/200
----------
Epoch: [7][10/100]	Loss 0.2812 (0.2554)	
Epoch: [7][20/100]	Loss 0.2598 (0.2708)	
Epoch: [7][30/100]	Loss 0.2486 (0.2681)	
Epoch: [7][40/100]	Loss 0.2440 (0.2630)	
Epoch: [7][50/100]	Loss 0.2386 (0.2585)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7086 (0.8029)	
Epoch: [1][20/100]	Loss 0.5576 (0.7143)	
Epoch: [1][30/100]	Loss 0.4224 (0.6365)	
Epoch: [1][40/100]	Loss 0.3716 (0.5762)	
Epoch: [1][50/100]	Loss 0.3399 (0.5315)	
Epoch: [1][60/100]	Loss 0.3200 (0.4980)	
Epoch: [1][70/100]	Loss 0.3105 (0.4716)	
Epoch: [1][80/100]	Loss 0.2937 (0.4503)	
Epoch: [1][90/100]	Loss 0.2916 (0.4331)	
Epoch: [1][100/100]	Loss 0.2922 (0.4185)	
Loss: 0.4185

Epoch 2/200
----------
Epoch: [2][10/100]	Loss 0.2744 (0.2820)	
Epoch: [2][20/100]	Loss 0.2816 (0.2821)	
Epoch: [2][30/100]	Loss 0.2703 (0.2811)	
Epoch: [2][40/100]	Loss 0.2769 (0.2800)	
Epoch: [2][50/100]	Loss 0.2778 (0.2788)	
Epoch: [2][60/100]	Loss 0.2722 (0.2776)	
Epoch: [2][70/100]	Loss 0.2733 (0.2769)	
Epoch: [2][80/100]	Loss 0.2711 (0.2760)	
Epoch: [2][90/100]	Loss 0.2715 (0.2752)	
Epoch: [2][100/100]	Loss 0.2730 (0.2744)	
Loss: 0.2744

Epoch 3/200
----------
Epoch: [3][10/100]	Loss 0.2617 (0.2633)	
Epoch: [3][20/100]	Loss 0.2617 (0.2638)	
Epoch: [3][30/100]	Loss 0.2584 (0.2622)	
Epoch: [3][40/100]	Loss 0.2587 (0.2613)	
Epoch: [3][50/100]	Loss 0.2732 (0.2617)	
Epoch: [3][60/100]	Loss 0.2617 (0.2620)	
Epoch: [3][70/100]	Loss 0.2549 (0.2617)	
Epoch: [3][80/100]	Loss 0.2565 (0.2614)	
Epoch: [3][90/100]	Loss 0.2508 (0.2612)	
Epoch: [3][100/100]	Loss 0.2596 (0.2609)	
Loss: 0.2609

Epoch 4/200
----------
Epoch: [4][10/100]	Loss 0.2625 (0.2569)	
Epoch: [4][20/100]	Loss 0.2558 (0.2557)	
Epoch: [4][30/100]	Loss 0.2535 (0.2559)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7193 (0.8155)	
Epoch: [1][20/100]	Loss 0.5828 (0.7286)	
Epoch: [1][30/100]	Loss 0.4545 (0.6531)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.6897 (0.7752)	
Epoch: [1][20/100]	Loss 0.5133 (0.6815)	
Epoch: [1][30/100]	Loss 0.4117 (0.6028)	
Epoch: [1][40/100]	Loss 0.3619 (0.5479)	
Epoch: [1][50/100]	Loss 0.3436 (0.5075)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7331 (0.8091)	
Epoch: [1][20/100]	Loss 0.5770 (0.7251)	
Epoch: [1][30/100]	Loss 0.4460 (0.6524)	
Epoch: [1][40/100]	Loss 0.3890 (0.5926)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7004 (0.7849)	
Epoch: [1][20/100]	Loss 0.5462 (0.6953)	
Epoch: [1][30/100]	Loss 0.4234 (0.6179)	
Epoch: [1][40/100]	Loss 0.3668 (0.5605)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.6940 (0.7820)	
Epoch: [1][20/100]	Loss 0.5217 (0.6946)	
Epoch: [1][30/100]	Loss 0.4048 (0.6150)	
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Training the 'DCEC' architecture

The following parameters are used:
Batch size:	600
Number of workers:	4
Learning rate:	0.01
Weight decay:	0
Scheduler steps:	20
Scheduler gamma:	0.1
Number of epochs of training:	10
Number of epochs of pretraining:	6

Data preparation
Reading data from: MNIST dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0


Initialize cluster centers:

Begin clusters training:
Epoch 1/200
----------
Epoch: [1][10/100]	Loss 0.7103 (0.7985)	
Epoch: [1][20/100]	Loss 0.5667 (0.7127)	
Epoch: [1][30/100]	Loss 0.4313 (0.6368)	
Epoch: [1][40/100]	Loss 0.3718 (0.5768)	
Epoch: [1][50/100]	Loss 0.3364 (0.5320)	
Epoch: [1][60/100]	Loss 0.3235 (0.4983)	
